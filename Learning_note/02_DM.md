# Data mining

> 以下为本人MOOC参加学习 国防科技大学 — 数据挖掘 课程第9次开课的笔记记录，若有疏漏错误，望友好指正！
> 课程链接：https://www.icourse163.org/course/NUDT-1461782176
> github资源链接：[GitHub - zyding1983/datamining: data mining class](https://github.com/zyding1983/datamining?tab=readme-ov-file)

## Introduction

### Big Data

Features

- High volume   —   数据量大   —   TB - ZB
- High velocity   —  数据产生速度快
- High variety    —   数据种类多样   —   structured - unstructured



### Task

三大任务

- 监督学习
- 非监督学习
- 关联规则挖掘





## Data

### 1. Types

1. 标称
2. 二进制：只有0 / 1两种状态的属性
   				- 对称：两种状态占比差不多
      	- 非对称：两种状态占比差异大
3. 序数
4. 区间标度





### 2. Visualization

1. 箱式图

   分析多个属性数据的==离散度差异性==

2. 直方图

   单个属性在各个区间的==变化分布==

3. 散点图

   两个属性数据的==关联关系==







### 3. 相似性

#### Matrix

1. 数据矩阵

   |        |                      |           |
   | ------ | -------------------- | --------- |
   | row    | 每行代表一个数据     | 共N个数据 |
   | column | 每列代表一个数据属性 | 共P个维度 |

2. 相异矩阵

   - 三角矩阵
   - N个数据点，记录两点间的距离

#### Calculation

1. 相似度：[0, 1] —— 1: 最相似
2. 相异度：[0, 1] —— 0: 最小相异度
3. 近邻性：相似度 / 相异度

#### Different data types

##### 1. 标称属性

简单匹配: m — 匹配次数
				 p — 属性总数
$$
d(i, j) = \frac{p+m}{p}
$$

##### 2. 二值属性

建立邻接表 — 计算

1. 邻接表

   分别计算两个数据的 0 & 1 状态个数，Σ = p

2. 对称
   $$
   d(i, j) = \frac{(i=1, j=0) + (i=0, j=1)}{p}
   $$

3. 非对称 (e.g. 病例)
   $$
   d(i, j) = \frac{(i=1, j=0) + (i=0, j=1)}{p - (i = 0, j = 0)}
   $$

4. Jaccard 系数
   $$
   sim_{Jaccard}(i, j) = \frac{(i=1, j=1)}{p - (i = 0, j = 0)}
   $$



##### 3. 数值属性

$D(x, y) = \left( \sum_{i=1}^{n} |x_i - y_i|^p \right)^{1/p}$

- $x = (x_1, x_2, \dots, x_n)$和$y = (y_1, y_2, \dots, y_n)$是两个$n$-维空间中的向量；
- $|x_i - y_i|$表示每个维度上的差的绝对值；
- $p$是一个非负实数，表示距离的阶数。

###### specialist

1. **当$p = 1$** 时，公式变为 **曼哈顿距离**（也称为城市街区距离）：
   $D(x, y) = \sum_{i=1}^{n} |x_i - y_i|$
   
2. **当$p = 2$** 时，公式变为 **欧几里得距离**：
   $D(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$
   
3. **当$p \to \infty$** 时，公式变为 **切比雪夫距离**（或最大值距离）：
   $D(x, y) = \max_{i} |x_i - y_i|$





### 4. 数据预处理

#### 数据清洗

##### a. 缺失值

1. **忽略**元组 (该行数据)

   **缺少类标号** (有监督训练集缺乏类标签)

2. **均值**填充

##### b.  噪声

箱线图检测离群数据：删除离群点

##### c. 不一致数据

1. 生日=“08/06/1988”
2. 计算推理、替换
   全局替换



#### 数据集成

将来自多个数据源的数据组合成一个连贯的数据源

##### a. 步骤

###### step 1 模式集成

链接不同数据源的方式 / 依据

###### step 2 实体识别问题

将同一实体的多条数据合成

- 同一实体在不同源中的属性值不同
- 原因：表述方式、尺度度量
- 例如：某人的中英文名，公制与英制单位 (某人的身高)

###### step 3 数据冲突与检测 (冗余信息)

1. 来源

   a. 相同的属性 / 对象可能以不同的名字在不同的数据库中

   b. 一个属性可能是另一属性的 “派生”

2. 检测

   相关性分析 / 协方差分析



##### b. 冗余信息的处理

1. 相关分析

   计算两个属性之间的相关程度，以减少数据属性个数，降低数据集大小

   - 离散
     **卡方测试 Chi-square test**:$X^{2}$越大，变量间越相关

     > 相关性 ≠ 因果关系

   - 连续
     **相关系数 / 皮尔逊相关系数**: 
     $$
     r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2 \sum_{i=1}^{n} (y_i - \bar{y})^2}}
     =\frac{\sum (pq)-n\bar{p}\bar{q}}{(n-1)\sigma_p\sigma_q}
     $$

     > - $x_i / p$和$y_i / q$分别是第$i$个样本点的两个变量的值
     > - $\bar{x}$和$\bar{y}$分别是$x$和$y$的样本均值，计算公式为 $\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i, \quad \bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i$
     > - $n$: 数据点的总数 / 元组数目
     > - $\sigma_p$ & $\sigma_q$: 各自标准差
     >
     > - $r$: [-1, 1]
     >   - $r = 1$：完全正线性相关
     >     $r = -1$：完全负线性相关
     >   - $r = 0$：两变量之间无线性相关性
     >   - $0 < r < 1$：正相关，但不是完全相关。
     >   - $-1 < r < 0$：负相关，但不是完全相关。
     > - $|r| < 0.4$: 低度线性相关
     >   $0.4 ≤|r| < 0.7$: 显著性相关
     >   $0.7 ≤|r| < 1$: 高度线性相关
     >

2. 协方差
   $$
   \text{Cov}(X, Y) = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})
    = E((p-\bar{p})(q-\bar{q}))
   $$

   > - 简化计算
   >   $\text{Cov}(p, q) = E(p·q) - \bar{p}\bar{q}$
   >   即两变量的各个值对应相乘，再减去各均值
   > - $x_i$ 和 $y_i$ 分别是第 $i$ 个样本点的两个变量的值
   > - $\bar{x}$ 和 $\bar{y}$ 分别是 $x$ 和 $y$ 的样本均值 $
   >   \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i, \quad \bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i
   >   $
   > - $n$ : 数据点的总数
   > - $r_{p,q}=\frac{\text{Cov}(p, q)}{\sigma_p\sigma_q}$
   > - +: $\text{Cov}(p, q) > 0$
   >   -: $\text{Cov}(p, q) < 0$
   >   独立: $\text{CovP}(p, q) = 0$
   >

   

#### 数据缩减 / 规约

##### a. 降维

减少数据属性数量

1. 原因
   高可用性数据稀疏
   不同属性带来不同规则
   机器学习中调整的参数 (特征权值) 增多
   可视化

2. 方法

   PCA主成分分析法

   > 将具有相关性的属性, **加权求和**进行**线性变换**成一个新的属性
   > e.g. $z_1=0.7x_1+0.63x_2+0.82x_3$

##### b. 数据抽样

1. 需保留原数据集的大部分结构

2. 方法

   简单随机抽样：放回 / 不放回

##### c. 数据压缩

降低数据质量 — 降低规模



#### 数据转换

##### a. 规范化

按比例缩放到一定空间

1. Max-Min
   $$
   X_{\text{norm}} = \frac{X - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
   $$
   
2. Z-score
   $$
   Z = \frac{X - \mu}{\sigma}
   $$

   > $\mu$: 均值
   >
   > $\sigma$: 标准差

3. Decimal Scaling
   $$
   X_{\text{norm}} = \frac{X}{10^j}
   $$

   > $j$: 使得最大值映射到 [-1, 1] 的最小整数



##### b. 离散化

连续的数值数据 — 离散的描述性数据

非监督离散方法

1. 等宽法
   将数值值域划分为等长的区间，每个区间对应一个离散值
2. 等频法
   根据数值出现的频数划分为样本数相等的区间
3. 聚类







## Classification

training set  ——  (learning algorithm)  ——  learn model  —— model

——  Apply model  —— (deduction)  —— Test set

### 1. Bayes

#### Concept

##### a. 计算

$$
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
$$

> $P(C|X)$: C 的后验概率 — 给定数据 $D$ 后，假设 $H$ 的概率
> $P(X|C)$: C 的似然概率 — 即在假设 $H$ 下，观察到数据 $D$ 的概率
> $P(C)$: C 的先验概率
> $P(X)$: X 的先验概率
>
> $X$: 待测试数据
> $C$: 假设类别
>
> **即现在存在一个实体 $X$ , 需要判断它属于类别 $C$ 的概率为多少**

##### b. 提出假设

存在类别 $C_1, C_2, ...$ , 判断观测数据 $X$ 属于哪一类别的概率最大

###### 极大后验假设 Maximum a posteriori: MAP

学习器在假设合集 $C$ 中找到 $X$ ==可能性最大的假设 $c$==，$c$ 称为极大后验假设
$$
c_{MAP} = max_{c \in C}P(c|X) = max_{x \in X}\frac{P(X|c) \cdot P(c)}{P(X)}=max_{x \in X}P(X|c) \cdot P(c)
$$
==即简化计算为只求分子最大值==

##### c. 实例

e.g. 一个收入中等、信用度良好、爱好游戏的青年顾客，是否会购买电脑？

$X$：顾客的各类特征属性集合 $<x_1, x_2, x_3, ..., x_n>$
		即需要计算 $P(<x_1, x_2, x_3, ..., x_n>|C)$, 即需计算联合概率 — 维度高, 数据稀疏, 开销大

###### 解决方法

假设 $<x_1, x_2, x_3, ..., x_n>$ 相互独立，即每个属性都是==**独立同分布**==的

称为==**朴素贝叶斯分类器**==

#### Case

##### 1

?: 一个收入中等、信用度良好、爱好游戏的青年顾客，购买电脑的概率

$P(青年|买)$, $P(收入中等|买)$, $P(爱好|买)$, $P(良好|买)$ 相互独立
		则：联合概率 $P(X|C)=\prod^{n}_{k=1}P(X_1|C) \cdot P(X_2|C) \cdot ... \cdot P(X_n|C)$
表示：买电脑的情况下出现符合 $x_n$ 特征的顾客的概率





##### 2

假设你是一名医生，根据一项检测结果为患者诊断。你的假设是患者可能患有某种疾病，你希望计算给定检测结果的情况下，这一假设的概率（即你希望计算 $P(疾病 | 检测)$）。

1. $P(疾病 | 检测)$ : **后验概率**——给定检测结果后，患者患病的概率
2. $P(检测 | 疾病)$ : **似然**——如果患者真的患有该疾病，获得这一检测结果的概率
3. $P(疾病)$ : **先验概率**——在检测之前，患者患病的概率（基于一般人群的发病率或已有的知识）
4. $P(检测)$ : **证据**——观察到检测结果的总体概率（无论患者是否患病）

通过计算似然，可以根据新的证据（检测结果）调整对假设（是否患病）的信念



#### 条件概率的M估计

如果有一个属性的类条件概率等于0,则整个类的后验概率就等于0。
很多情况下数据集不能覆盖各种“属性的组合情况”,为了规避这个问题我们引入条件概率的m估计。
$$
P(x_i|y_i)=\frac{n_c + mp}{n + m}
$$


> $n$：类 $y_i$ 中的实例总数
> $n_c$：类 $y_i$ 的训练样例中取值$x_i$的样例数
> $m$：称为等价样本大小的参数
> $p$：用户指定的参数

---

如果训练集中不存在 $y_j$ 类 (即n=0),则$P(x_i|y_i)=p$。
因此 $p$ 可以看作是在类 $y_j$ 的记录中观察属性值$x_i$的先验概率。等价样本大小决定先验概率 $p$ 和观测概率 $n_c/n$ 之间的平衡。
使用m估计方法,m=3, p=1/3,则条件概率不再是0:
m一般取和n相等的大小n是有三个yes nc是三个yes里没有已婚的
P(婚姻状况=已婚|Yes)=(0+3x1/3)/(3+3)=1/6

---

以上存在纰漏，未进行格式化，需自行查证。



### 2. 决策树 decision tree

#### Concept

1. 测试结点
   作为判断条件的属性
2. 分直
   根据条件属性取值选取的路径
3. 叶子
   使判断终止的结论



#### How to build a decision tree

##### a. Hunt

1. $D_t$ 中所有记录都属于 $y_t$, 则 $t$ 为叶结点，用 $y_t$ 标记
2. 若 $D_t$ 中包含属于多个类的记录，则选择一个属性测试条件，将记录划分成较小的子集
3. 对测试条件的每个输出，创建一个子节点，并根据测试结果将 $D_t$ 中的记录分布到子结点中。
4. 对每个子节点，递归调用 1. — 3. 
5. …… unknown



##### b. 划分数据

1. 划分路数
   **多路划分**：数据有几种属性值，则划分为几路
   **二元划分**：考虑 K 个属性所有的不同组合状态
2. 属性类型: 标称、序数、连续
3. 二元划分序数类型数据: 需要保留数据的有序性 `(e.g. {[small, medium], [Large]}, {[large, medium], [small]}, 不可划分为{[small, Large], [medium]})`
4. 二元划分连续类型数据: 考虑所有划分点，选择一个最优划分点 `(A < v) or (A >= v)`
5. 多路划分连续数据: 连续数据离散化后划分

 



#### Construction

##### a. best

纯性越高，类分布越倾斜，划分结果越好
e.g. $c_0=9$，$c_1=1$ 纯性高

##### b. Entropy   熵

衡量数据中的不确定性或无序程度

> 熵值高，数据混乱，纯度低
>
> 熵值低，纯度高

###### Calculate Entropy

The entropy $H(S)$ of a dataset $S$ is calculated using the formula: 

$$
H(S) = -\sum_{i=1}^{n} p_i \log_2(p_i)
$$

> $p_i$: the probability of class $i$ in the dataset. 
>
> e.g.  $c_0=9$，$c_1=0$ 
> 		$E = -0 \log 0 - \frac{9}{9} \log \frac{9}{9} = 0$ 

###### Calculate Conditional Entropy

The formula for conditional entropy $H(Y|X)$ is: 

$$
H(Y|X) = -\sum_{x \in X} P(X=x) \sum_{y \in Y} P(Y=y|X=x) \log_2(P(Y=y|X=x))
$$

##### c. Information Gain   信息增益

###### concept

衡量一个特征对于数据集的类别标签所提供的信息量。即量化某特征在分类一堆数据方面的有效程度。

###### calculation

该类别信息增益 = 原始信息熵 - 按该类别分类信息熵

信息增益越大，按该类别划分的结果越纯



##### d. Gini  系数

较低的基尼指数表明组内更加同质，意味着大多数数据点属于同一类别，而较高的基尼指数则表明组内纯性越低。

即 GINI 越低，数据越纯
$$
G = 1 - \sum_{i=1}^{n} (p_i)^2
$$


#### method

过拟合：训练集少，模型过于复杂

欠拟合：训练集多，模型过于简单

##### a. ID3

递归 max ( $Gain(S, A)$) 

> $S$: 数据集 $S$
>
> $A$: 类别 $A$
>
> 表示递归地选取数据集中的各个属性计算信息增益，并选取Gain最大的 $A$ 作为根节点



##### b. C4.5



##### c. CART



##### d. 剪枝

###### Pre-pruning 事前剪枝

在从上到下构建决策树的过程中，若某一层计算的**信息增益、Gini指标 ≤ 设定阈值** / **元组数目低于阈值**，则停止此次划分

###### Post-pruning 事后剪枝

从完全生长的树中减去树枝
e.g. 用新的测试集对决策树的分支计算错误率，减去错误率高的分支



#### 二元划分连续类型数据示例

若一组连续数据：{1，2，3，4，5，6，7，8，9，10，……}

从最小值开始建立划分区间，计算左右两边各自的信息增益，从中选取 $Gain$ 最大的一个分割区间作为划分点





### 3. 逻辑回归

逻辑回归函数 (sigmoid 函数？？？— 请自行查证)
$$
f(x) = \frac{e^x}{1+e^x}
$$
曾用于疾病监测，$p_i$ 为患病概率
$p_i=P(y_i=1 | x_{i1}, x_{i1}, ... , x_{ip})$

将 $x_i$ 作为因变量代入 $f(x)$ 可得 $p_i=\frac{exp(\alpha+\beta_1{x_{i1}}+\beta_2{x_{i2}}+...+\beta_p{x_{ip}})}{1+exp(\alpha+\beta_1{x_{i1}}+\beta_2{x_{i2}}+...+\beta_p{x_{ip})}}$

通过数学变换得：
$$
ln\frac{p_i}{1-p_i}=\alpha+\beta_1{x_{i1}}+\beta_2{x_{i2}}+...+\beta_p{x_{ip}}
$$


得到 **逻辑式变换**：
$$
Logit(p_i)=\alpha+\beta_1{x_{i1}}+\beta_2{x_{i2}}+...+\beta_p{x_{ip}}+\epsilon_i
$$

> 若 $Logit(p_i) > 0$, 则为正类
>
> 若 $Logit(p_i) < 0$, 则为负类



#### 优势比 OR

衡量危险因素作用大小的**比数比例**指标

公式
$$
OR_j=\frac{P_1/1-P_1}{P_0/1-P_0}
$$

> **表示自变量变化时，发病概率得变化情况**
>
> $P_1, P_0$: 在 $x_j$ 取值为  $c_1,c_0$ 的发病概率
> 			 即某个特征在取两个不同的值时的发病概率
>
> 可以看出单个变量对整体发病情况的影响

由逻辑变换式得
$$
lnOR_j=ln[\frac{P_1/1-P_1}{P_0/1-P_0}]=logit(P_1)-logit(P_0)=\beta_j(c_1-c_0)
$$
最终得
$$
OR_j=exp[\beta_j(c_1-c_0)]
$$

> 若 $c_1-c_0=1$:
>
> $\beta_j=0, OR_j=1$: 该特征无作用
>
> $\beta_j>0, OR_j>1$: 该特征是危险因子
>
> $\beta_j<0, OR_j<1$: 该特征是保护因子



#### 正则化

$$
Logit(p_i)=\alpha+\beta_1{x_{i1}}+\beta_2{x_{i2}}+...+\beta_p{x_{ip}}+\epsilon_i
$$

权值系数在数值上越小越好，可以抵抗数据得扰动

##### 正则化表达式

1. $L_1=\sum_{i=0}|\omega_i|$ 稀疏编码，得到的系数非0即1，适用于减低维度
2. $L_2=\sum_{i=0}^m\omega_i^2$ 岭回归，较强的概率意义

最终得到 **误差函数 = 真实值与预测值的平方和 + $\lambda{L_1}$ / $\lambda{L_2}$**

加入正则化表达式的误差函数称为 **惩罚项**

	- 可以在学习到大权值时得到的小误差，通过加上正则化表达式的值之后最终变大，进而使得求解的权值尽可能相对较小



#### 数值优化

若各个维度/特征在数值上的差异很大，则需要进行数值归一化



#### 训练方法优化

梯度下降法: SGD & L-BFGS

##### SGD

1. 需要提前将数值归一化

2. 支持 $L_1, L_2$

3. 不支持多分类

4. 随机从训练集选取数据

   

##### L-EFGS

1. 需要均值归一化，算法包含方差归一化
2. 支持 $L_2$
3. 收敛快，考虑二阶导数
4. 支持多分类
5. 所有数据都参加训练







### 4. 关联规则挖掘

#### Concept

假设所有记录如下所示

|  ID  |            Items            |
| :--: | :-------------------------: |
|  1   |     {a, b, i, o, g, p}      |
|  2   |     {a, e, c, f, g, j}      |
|  3   | {b, c, d, e, m, h, j, l, f} |
|  4   |    {a, e, l, m, f, g, i}    |
|  5   |  {a, c, e, h, j, g, k, l}   |



##### a. association analysis 关联分析

1. 用于发现数据中的联系

2. 用 **关联规则** 或 **频繁项集** 的形式表示

   ```python
   Rules Discovered:
   	{a} --> {b}
       # 即a和b间存在关联
   ```

3. 关联规则
   反映一个事物与其他事物之间的相互依存和联系性

   该规则中的一个事物可以**预测与它关联的其他事物发生**

##### b. Frequent Itemset 频繁项集

1. Itemset 项集

   即项的集合 (0 - n)	`e.g.{a, b, c}`
   k项集：有 k 项

2. Support count ($\sigma$) 支持度计数

   包含特定项集的事务个数 —— 即在所有记录中，包含了 `{}` 内的集合有几个
   `e.g. σ({a, b, c}) = 2`

3. Support 支持度

   包含项集的事物数与总事务数的比值

   `e.g. s({a, b, c}) = 2/5`

4. Frequent Itemset 频繁项集

   满足最小支持度阈值 (minsup) 的所有项集
   需要自行根据经验指定

5. Maximal Frequent Itemset 最大频繁项集

   直接超集都不是频繁项集的频繁项集
   **直接超集**：$B \sub A$ 且 二者只相差一个元素，则 A 是 B 的直接超集

##### c. Association Rule 关联规则

1. 形式：`X -> Y` (蕴含表达式)
   			X, Y 为不相交项集
      			`e.g. {a, c} -> {b}`

2. **Support (S) 支持度**: 

   确定项集的频繁程度
   $$
   S=\frac{X,Y在所有项集中都出现的个数}{项集的总个数}
   $$
   
3. **Confidence (C) 置信度**: 

   确定 Y 在包含事务 X 的事务中出现的频繁程度 —— 即 X 确定出现的情况下，Y 出现的概率 ==(可视作条件概率 P(Y|X))==

   需要用一个项集预测另一个项集，通常需要考虑频繁二项集及以上
   $$
   C=\frac{X,Y在所有项集中都出现的个数}{X在所有项集中出现的个数}
   $$

##### d. 关联规则挖掘问题

找出**支持度 >= `minsup`**(支持度阈值) 且 **置信度 >= `minconf`** (置信度阈值) 的所有规则

1. 根据 `minsup` 找到所有支持度都高于 `minsup` 的项集

   找到的项集称为 **频繁项**
   频繁项内有 k 个项，则称为频繁 k 项集 (见 [Frequent Itemset 频繁项集](##### b. Frequent Itemset 频繁项集) )

2. 根据 `minconf` 查看频繁二项集及以上的项的置信度是否**高于** `minconf`

   由于项集内包含了两个以上的项，因此需分别计算置信度，即：

   - 左侧项 => 右侧项，即 X 出现的情况下，Y 出现的置信度
   - 右侧项 => 左侧项，即 Y 出现的情况下，X 出现的置信度



#### Frequent Itemset Generation

找出(最大频繁项集) / (满足最小支持度阈值的所有项集, 即频繁项集) ？

##### a. `Apriori`

减少候选项集的数量 (宽度)

1. 先验原理

   - 项集频繁 => 所有**子集频繁**
     e.g. 若 {A, B, C} 频繁， 则 {A, B}, {B, C}, …… 都频繁
   - 项集非频繁 => 所有**超集非频繁**
     e.g. 若 {A, B} 非频繁， 则 {A, B, C}, {A, B, D}, …… 都非频繁

2. **算法具体过程**

   - 给定包含各种项的数据记录
   - 找出所有 **k项集** (k从1开始)
   - 计算所有 k 项集的**支持度**
   - 剔除支持度 < `minsup` 的所有 k 项集  ——  提前剪枝
   - 将 k 项集相互**连接**成不重复的 k+1 项集 [即1项集合并成2项集，2项集合并成3项集……]
     项的连接存在规则，避免两两连接产生多个候选项
   - 重复 2 — 5

   > ATT: 为了方便计算连接，默认项都按照字典序排序，即A, B, C, ……

3. 项的字典序

4. 项的连接规则

   - 对任何2个需要连接的项集，同时**去掉**这两个项集的**尾项** (已按字典序排序完成)
   - 若**剩余项一致**，则**可连接**
   - 否则不可连接

5. drawback

   候选项数目可能在项的数目较大的情况下，呈现爆炸式增长 (宽度优先)





##### b. `FPGrowth`

减少比较次数 (深度)

1. 基本思想
   - 扫描数据库两遍，构造频繁模式树 (FP-Tree)
   - 自底向上递归产生频繁项集
   - FP树：输入数据的压缩表示
     通过逐个读入事务，并把每个事务映射到FP树中的一条路径来构造
   
2. 构造模式增长树 (条件模式树)

   - 根据所有项集计算所有项的支持度

   - 根据 支持度 >= `minsub` 得到所有**频繁 1-项集**, 并填充头表的前两列

   - 将所有项集中的**非频繁项删除**，并将项集中的项按照项的支持度大小排序，如下：

     |  ID  |   ordered Items    |
     | :--: | :----------------: |
     |  1   |       {a, g}       |
     |  2   | {a, e, g, c, f, j} |
     |  3   |  {e, c, f, j, l}   |
     |  4   |  {a, e, g, f, l}   |
     |  5   | {a, e, g, c, j, l} |
   
   - 完善头表，建立模式增长树 (条件模式树)
     每一条路径：一条数据
     项后的数值：该路径上项出现的次数
   
   <img src="D:\A_assignment\work\03\3_1\A_DataMining\notes\images\001.jpg" style="zoom:40%;" />
   
3. 产生条件模式库

   - 从 FP-Tree 的头表开始 (猜测老师未讲解a项，是由于其条件模式基为 **空**)
   - 按照每个频繁项的连接，遍历 FP-Tree
   - 列出所有能够到达该项的所有**前缀路径**，即为 **条件模式基** (不清楚是否需要按照自左向右的顺序填写库)
   - 例如：条件模式基格式 —— **前缀路径: 到达该项的次数**

   | item | cond. pattern base(conditions — 条件模式基) |
   | ---- | :------------------------------------------ |
   | e    | a: 3                                        |
   | g    | a:1, ae: 3                                  |
   | c    | aeg: 2, e: 1                                |
   | f    | aeg: 1, aegc: 1, ec: 1                      |
   | j    | aegcf: 1, aegc: 1, ecf: 1                   |
   | l    | aegf: 1, aegcj: 1, ecfj: 1                  |

4. 建立条件模式子树 (模式增长子树)

   - 对每个项的模式库，筛选出前缀路径 支持度 >= `minsub` 的项集，即可得到 **模式增长子树**
     `e.g. c-条件模式库最终得到的项集为 {a, e, g}`
   - 若该模式增长子树是一条路径，那么就结束该子树的增长，路径上的项组合起来的项集称为 **最大频繁项集** (????? 老师是这么说的)
     可以产生以下频繁模式 (或称作频繁项集)：`c, ac, ec, gc,aec, agc, egc, aegc`

5. strengths

   完备性

   紧密性



#### Rule Generation

从上一步的频繁项集中提取所有高置信度的规则，即**强规则**

##### 剪枝算法

1. 同一频繁项集的关联规则，若**规则后件**满足**子集**关系，则这些规则**置信度**间满足**反单调性**

2. 如下
   若有频繁项集 {A, B, C, D}, 由置信度计算公式可知，规则置信度 C 满足：
   $$
   c(ABC → D) >= c(AB → CD) >= c(A → BCD)
   $$

3. 剪枝操作

   若 $c(ABC → D)$ 已经不满足置信度阈值，那么其后的所有 $c(AB → CD)， c(A → BCD)$…… 等子集，子集的子集都无法满足，即可进行剪枝



#### Pattern Evaluation

lift 方法计算
$$
lift = \frac{P(A \cup B)}{P(A)P(B)}
$$
若 lift > 1，则该规则有意义；否则无意义

---

本篇更新中。。。









## Clustering

对没有类标号的数据进行划分称为**聚类**

- 把数据对象集合按照**相似性**划分成多个子集的过程
- 每个子集是一个簇 `(cluster)`，使得**簇中的对象彼此相似**，但与其他簇中的对象不相似



### 1. 划分方法

将有 N 个对象的数据集 D 划分成 K 个簇，K <= N ，满足：

- 每个簇至少包含一个对象
- 每个对象属于且仅属于一个簇
- 



#### 启发式方法

$$
E = \sum^{k}_{i=1} \sum_{p \in C_i}(d(p, c))
$$

> $p$: 数据集中的对象
> $c$: 簇中心

最终结果需要 $E$ 尽可能小

#### a. K-means

- 每个簇用簇中对象的 **均值** 表示
- 基于 **质心**



##### 步骤

1. 指定簇的数量 (可通过画散点图大致判断)
2. 首次划分时随机选取两个均值点作为簇中心
3. 计算每个点到均值点的距离
4. 比较距离 d，该点属于取得最小距离时的均值点所在簇
5. 更新簇的均值点
6. 重复3. — 5. ，直到收敛 (均值中心变动很小 / 每个点到均值点的距离变动很小)

相同为0，不同为1



##### 优缺点

- 常常终止于局部最优
- 对噪声和异常数据敏感
- 不同初始值结果 (随即初始种子 `random_state`) 可能不同
- 不适合发现非凸面形状簇，适用于球状簇





##### 优化改进

- K-modes
  1. 选择离散数据中的众数作为均值点
  2. 解决 K-means 只适用于**连续**数据的问题
- K-means++
  1. 尽可能选择距离相隔远的点作为初始种子结点
  2. 解决**初始点**选择对聚类结果敏感性的问题
- K-中心点
  1. 选择簇中位置最中心的实际对象作为参照点
  2. 划分原则：所有对象与其参照点之间的相异度之和最小
  3. 解决**离群点**对聚类结果的影响



#### b. K-medoids

- 每个簇用接近簇中心的 **一个对象** 表示
- 基于 **代表对象**





### 2. 层次方法

### 3. 基于密度的方法

### K-summary





为什么信息增益率比信息增益有效

为什么

of3

of2 基于密度 相对密度的计算

of1 找k最近邻





final exam review https://cloud.tencent.com/developer/article/2093242







## SVM



1. The task requires using this dataset to handle a custom multi-class classification problem by specifying a few feature values for custom classification, with no more than 3 classes.
2. Split the dataset into training and testing sets, and obtain the testing accuracy.
3. Use SVM with different kernel functions for testing and obtain the testing accuracy.

```
density;
residual sugar;
alcohol;
total sulfur dioxide;
chlorides;
free sulfur dioxide;
volatile acidity;
citric acid

1"fixed acidity";
2"volatile acidity";
3"citric acid";
4"residual sugar";
5"chlorides";
6"free sulfur dioxide";
7"total sulfur dioxide";
8"density";
9"pH";
10"sulphates";
11"alcohol";
"quality"
[7, 3, 10, 6, 4, 5, 1, 2]

```
